<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="icon" href="../../favicon.ico">

    <title>TEBIO-Fit - About</title>

    <link rel="stylesheet" href="bootstrap/3.3.6/css/bootstrap.css">
    <link rel="stylesheet" href="starter-template.css">

    <link rel="stylesheet" type="text/css" href="site.css">


</head>

<body>

<nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar"
                    aria-expanded="false" aria-controls="navbar">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="">TEBio-Fit</a>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
            <ul class="nav navbar-nav">
                <li><a href="about.html">About</a></li>
                <li><a href="mailto:james@jamesscottbrown.com">Contact</a></li>
            </ul>
        </div><!--/.nav-collapse -->
    </div>
</nav>

<div class="container">




    <div class="starter-template">
        <h1>About TEBio-Fit</h1>

        <p><blockquote>Reasoning about sets of nonlinear dynamical systems is hard. Good visualization systems can make it easier.</blockquote></p>

        <p>TEBio-Fit provides interactive visualization to facilitate parameter-estimation and model comparison
            for general non-linear models.

            </p>

        <p>Currently, it is focused on displaying the results produced by
            <a href="http://www.theosysbio.bio.ic.ac.uk/resources/abc-sysbio/">ABC-SysBio</a>, a tool for likelihood-free
            parameter inference and model selection in dynamical systems that uses the Approximate Bayesian Computation
            Sequential Monte Carlo (ABC SMC) approach described below. However, it could be extended to view the results
            produced form other tools (for example, for the results of applying MCMC to models for which a likelihood
            function is available).
        </p>


        <h2>Design Principles</h2>

        <p>TEBio-Fit was designed with several principles in mind:</p>
        <ul style="text-align: left;">
            <li><em>easy setup</em>: the viewer run in any modern web-browser, on any platform with nothing to install. Pre-processing requires python scripts with minimal dependencies.</li>
            <li><em>modularity</em>: code to visually compare two SBML models is a separate python module that can be re-used in other packages. Code to perform visualisation is entirely independent of code that performs inference</li>
            <li><em>freely licensed</em>: BSD license imposes few restrictions on re-use</li>
            <li><em>interactivity</em> allows the user to see all the data, yet also focus only on what is important to them at a particular moment</li>
            <li>figures are directly integrated with explanations of what they show. The purpose of this is two-fold: it help the novice user by providing explantions at the point of need, avoiding them having to refer back to documentation, and it produces a self-contained report that can be presented to a collaborator or supervisor who may be unfamiliar with the tools used.</li>
            <li>easy export of images: figures can be exported, and optionally edited in third-party applications</li>
        </ul>


        <h1>Theory</h1>

        <p>Suppose we have some initial idea of what values the parameters in a model might have, which we can express as a prior probability distribution. We then perform an experiment, obtaining measurements $x_0$. We now want to use these measurements to update our knowledge about the likely values of the parameters. </p>

        <p>The way to do this is to obtain the <i>posterior distribution</i> by applying Bayes' rule: $$ \underbrace{p(\theta | x_0)}_{posterior} = \frac{ \overbrace{p(x_0 | \theta)}^{likelihood}  \overbrace{p(\theta)}^{prior} }{ p(x_0) } $$</p>

        <p>For sufficiently simple models, we can find an analytical expression for the posterior probability distribution function. More commonly, this is impossible, but we can draw samples from it using a Monte Carlo method. For more complicated models, we may not even be able to obtain an expression for the likelihood function, but if we can perform simulations we can apply an Approximate Bayesian Computation (ABC) method.</p>

<br />


        <p>Three Monte-Carlo methods, and their ABC equivalents, are summarised in the table below. For clarity, the
            prior is denoted by $\pi(\theta)$</p>
        <table class="table-bordered" style="text-align: left;">
            <thead><tr><th>Approach</th><th>Monte-Carlo Method</th><th>ABC Method</th></tr></thead>
            <tbody>

            <tr>
                <td> </td>
                <td>
                    <ul>
                        <li>Given $x$ and $\theta$, we can evaluate the likelihood $\color{blue}{ p(x_0 | \theta) }$.</li>
                        <li>Samples exact posterior $p(\theta | x_0 )$ </li>
                    </ul>
                </td>

                <td>
                    <ul>
                        <li>Given $\theta$, we cannot evaluate $\color{blue}{ p(x_0 | \theta)}$, but can draw a sample
                            from $p(x | \theta^*)$ by performing a simulation, and accept $\theta^*$ only if this is
                            sufficiently close to $x_0$ </li>
                        <li>Samples from the approximate posterior $p(\theta | \color{red}{d(x^*, x_0)} \leq \epsilon )$, where
                            $\epsilon$ is a parameter that determines the closeness of the approximation, and $d$ is a
                            distance function </li>

                    </ul>

                </td>
            </tr>



            <tr>
                <td>Rejection Method</td>

                <td>
                    <ul>
                        <li>Sample $\theta^*$ from $\pi(\theta)$</li>
                        <li>Sample $u$ from $U(0, 1)$</li>
                        <li>If $u < \color{blue}{p( x_0 | \theta^*)}$, accept $\theta^*$</li>
                    </ul>
                </td>

                <td>
                    <ul>
                        <li>Sample $\theta^*$ from $\pi(\theta)$</li>
                        <li>Sample $x^*$ from $p(x | \theta^*)$, by simulation</li>
                        <li>If $\color{red}{d(x_0, x^*)} \leq \epsilon$, accept $\theta^*$</li>
                    </ul>
                    <a href="http://dx.doi.org/10.1007/978-3-642-18743-8_5">Approximate Bayesian Computation and MCMC</a>

                </td>
            </tr>

            <tr>
                <td>MCMC</td>

                <td>
                    <p>Initialise $\theta^{(1)}$</p>
                    <p><b>For</b> $i=1 \ldots N$:</p>
                    <ul>

                        <li>Sample $\theta^*$ from the proposal distribution $Q(\theta^* | \theta^{(i)})$</li>
                        <li>Sample $u$ from $U(0, 1)$</li>
                        <li><b>If</b> $u < \min \left( 1, \frac{ \color{blue}{P(x_0 |\theta^*)} \pi(\theta^* ) Q(\theta^*  | \theta^{(i)} )}{ \color{blue}{P(x_0 | \theta^{(i)})} \pi(\theta^{(i)}) Q(\theta^{(i)} | \theta^* ) } \right)$,
                            then $\theta^{(i+1)} = \theta^*$; otherwise $\theta^{(i+1)} = \theta^{(i)}$</li>
                    </ul>
                </td>

                <td>
                    <p>Initialise $\theta^{(1)}$</p>
                    <p><b>For</b> $i=1 \ldots N$:</p>
                    <ul>
                        START
                        <li>Sample $\theta^*$ from the proposal distribution $Q(\theta^* | \theta^{(i)})$</li>
                        <li>Sample $x^*$ from $p(x | \theta^*)$, by simulation</li>
                        <li><b>If</b> $\color{red}{d(x_0, x^*)} > \epsilon$, then <b>GOTO START</b></li>
                        <li><b>If</b> $u < \min \left( 1, \frac{\pi(\theta^* ) Q(\theta^*  | \theta^{(i)} )}{ \pi(\theta^{(i)}) Q(\theta^{(i)} | \theta^* ) } \right)$,
                            then $\theta^{(i+1)} = \theta^*$; otherwise $\theta^{(i+1)} = \theta^{(i)}$</li>
                    </ul>

                    <a href="http://dx.doi.org/10.1073/pnas.0306899100">Markov chain Monte Carlo without likelihoods</a>
                </td>
            </tr>

            <tr>
                <td>SIS</td>

                <td>
                    <p>For $t = 0 \ldots T$:</p>
                    <ul>
                        <p>For $i = 1 \ldots N$:</p>
                        <ul>
                            START
                            <li> <b>If</b> $t=0$, sample $\theta^{**}$ independently from $\pi(\theta)$ </li>
                            <li><b>If</b> $t>0$, sample $\theta^{**}$ from previous population ${\theta^i_{t-1}}$ with weights $w_{t-1}$, then obtain $\theta^{**}$ by perturbing with kernel $K_t$</li>
                            <li><b>If</b> $\pi(\theta^{**}) = 0$, then <b>GOTO START</b></li>
                            <li>Set $\theta_t^{(i)} = \theta^{**}$</li>
                            <li><b>If</b> $t=0$, $w_t^{(i)} = 1$; otherwise $$w_t^{(i)} = \frac{  \color{blue}{P(x_0 | \theta_t^{(i)})} \pi(\theta_t^{(i)})  }{ \sum_{j=1}^{N} w_{t-1}^{(j)} K_t(  \theta_t^{(i)} | \theta_{t-1}^{(j)}  ) }$$</li>
                        </ul>
                    </ul>
                </td>

                <td>
                    <p>For $t = 0 \ldots T$:</p>
                    <ul>
                        <p>For $i = 1 \ldots N$:</p>
                        <ul>
                            START
                            <li> <b>If</b> $t=0$, sample $\theta^{**}$ independently from $\pi(\theta)$ </li>
                            <li><b>If</b> $t>0$, sample $\theta^{**}$ from previous population ${\theta^i_{t-1}}$ with weights $w_{t-1}$, then obtain $\theta^{**}$ by perturbing with kernel $K_t$</li>
                            <li><b>If</b> $\pi(\theta^{**}) = 0$, then <b>GOTO START</b></li>
                            <li>Sample $x^*$ from $p(x|\theta^{**})$ by simulation</li>
                            <li><b>If</b> $\color{red}{d(x^*, x_0)} \geq \epsilon_t$, then <b>GOTO START</b></li>
                            <li>Set $\theta_t^{(i)} = \theta^{**}$</li>
                            <li><b>If</b> $t=0$, $w_t^{(i)} = 1$; otherwise $$w_t^{(i)} = \frac{  \pi(\theta_t^{(i)})  }{ \sum_{j=1}^{N} w_{t-1}^{(j)} K_t(  \theta_t^{(i)} | \theta_{t-1}^{(j)}  ) }$$</li>
                        </ul>
                    </ul>
                    <a href="http://dx.doi.org/10.1098/rsif.2008.0172">Approximate Bayesian computation scheme for parameter inference and model selection in dynamical systems</a>
                </td>
            </tr>



            </tbody>
        </table>

        <p>Note that there is a progression here: the rejection method draws each sample independently of its
            predecessors; the MCMC method uses the previous sample when drawing the next; and the SIS method keeps track of a population of samples at each time-point.</p>

        <p>The SIS method also makes use of a series of thresholds $\epsilon_t$: these follow a decreasing schedule, so
            that the set of samples from successive populations are drawn from closer approximations to the true posterior.</p>


    </div>

</div><!-- /.container -->



<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>